{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.2: tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x105ad5e50>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/\"\n",
    "       \"ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "\n",
    "urllib.request.urlretrieve(url, \"the-verdict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 20479\n",
      "First 100 characters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    verdict = f.read()\n",
    "\n",
    "\n",
    "print(f\"Number of characters: {len(verdict)}\")\n",
    "print(\"First 100 characters:\")\n",
    "verdict[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some', 'example', 'sentence', '.', 'Thanks', 'for', 'joining', 'us', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split by space\n",
    "result = re.split(r'([,.!]|\\s)', \"Some example sentence. Thanks for joining us!\")\n",
    "\n",
    "[item for item in result if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'matey',\n",
       " '!',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'some',\n",
       " 'text',\n",
       " '?',\n",
       " 'I',\n",
       " 'think',\n",
       " '?',\n",
       " 'Is',\n",
       " 'this',\n",
       " 'thing',\n",
       " '--',\n",
       " 'on',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a little more complex\n",
    "\n",
    "tmp = \"Hello, matey! Here is some text? I think? Is this thing -- on?!\"\n",
    "\n",
    "result = re.split(r'([,.:;!_\"()?]|--|\\s)', tmp)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets apply this to the Edith Wharton text\n",
    "\n",
    "preprocessed = re.split(r'([,.:;!_\"()?\\']|--|\\s)', verdict)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.3: Converting tokens into token IDs\n",
    "\n",
    "We have a bunch of tokens, but we need to convert them into unique IDs. Out the gate this is easy enough to do. Get the tokens and alphabetize them. Remove duplicates and gives each one a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130 unique words in the text\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"{len(all_words)} unique words in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx for idx, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n",
      "?: 10\n",
      "A: 11\n",
      "Ah: 12\n",
      "Among: 13\n",
      "And: 14\n",
      "Are: 15\n",
      "Arrt: 16\n",
      "As: 17\n",
      "At: 18\n",
      "Be: 19\n",
      "Begin: 20\n",
      "Burlington: 21\n"
     ]
    }
   ],
   "source": [
    "for word, idx in vocab.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all this logic into a class\n",
    "\n",
    "class defined in `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SimpleTokenizerV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[642, 180, 663]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.encode(\"look at me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some goofy sentences using random IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forehead absurdity sign unusual\n",
      "true It resented hooded\n"
     ]
    }
   ],
   "source": [
    "print(tokenize.decode([459, 123, 888, 1050]))\n",
    "print(tokenize.decode([1035, 56, 837, 554]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.4: Adding special context tokens\n",
    "\n",
    "BUT we run into issues when a never before seen word shows up. We handle this by adding some additional handlers to the vocabulary. `|unk|` is used when we don't know a word (we can make sure the code is retrieved using the `.get` method for dicts). In addition, we can make sure we tell the model that an end of document has been reached using another one like `|end of text|`. \n",
    "\n",
    "There are many other different context tokens we can use - these are just a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'younger')\n",
      "(1, 'your')\n",
      "(2, 'yourself')\n",
      "(3, '<|endoftext|>')\n",
      "(4, '<|unk|>')\n"
     ]
    }
   ],
   "source": [
    "for item in enumerate(list(vocab.keys())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In V2, we add the special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SimpleTokenizerV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_v2 = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 1131]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 unknown words\n",
    "tokenize_v2.encode(\"BOOM POW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So here I am, it's in my head <|endoftext|> Eating seeds is a past time activity\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing end of text - these are 2 songs so we'd want to separate them\n",
    "\n",
    "text1 = \"So here I am, it's in my head\"\n",
    "text2 = \"Eating seeds is a past time activity\"\n",
    "\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|unk|> here I am, it' s in my head <|endoftext|> <|unk|> <|unk|> is a past time activity\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_v2.decode(tokenize_v2.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.5: Byte pair encoding (BPE)\n",
    "\n",
    "A much more sophisticated tokenization approach used in the training of the GPT series. It was first described for text compression in 1994 by Philip Gage! Quite complex to implement, so we just import it via `tiktoken`. It breaks down tokens into pieces so we can successfully embed new, unknown tokens. We don't have a bunch of `<|unk|>` tokens if we have many new words.\n",
    "\n",
    "[From wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding), the algorithm works by finding common adjacent pairs of characters with unused placeholder bytes. It continues to do this until there are no more adjacent pairs appearing more than once or until a desired vocabulary size is reached.\n",
    "\n",
    "The process ends up with a vocabulary of every individual letter present and the various combinations that may be plentiful. [This video](https://huggingface.co/learn/nlp-course/en/chapter6/5) has a really nice step by step process of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tiktokenize = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496,\n",
       " 11,\n",
       " 466,\n",
       " 220,\n",
       " 345,\n",
       " 588,\n",
       " 8887,\n",
       " 30,\n",
       " 220,\n",
       " 50256,\n",
       " 554,\n",
       " 262,\n",
       " 4252,\n",
       " 18250,\n",
       " 8812,\n",
       " 2114,\n",
       " 286,\n",
       " 617,\n",
       " 20035,\n",
       " 27271,\n",
       " 13]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\"Hello, do  you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.\")\n",
    "\n",
    "integers = tiktokenize.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do  you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktokenize.decode(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the nonsense word made it back - this is due to BPE algorithm that iteritively generates words.\n",
    "\n",
    "### section 2.6 Data Sampling with a Sliding Window\n",
    "\n",
    "With all of this text, we'll want to build training examples and have a way to load them into our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can this Mac use GPUs?: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Can this Mac use GPUs?: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "from utils import create_dataload_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of tokens is slightly larger than the original approach that took the full words. This is because of the components being counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(verdict)\n",
    "len(enc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the training and target sets. Our context window is the size of the text string given for context. Relatively short right now so we can get a sense of what is going on visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "# sample for more interesting text\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "context_window = 4\n",
    "\n",
    "x = enc_sample[:context_window]\n",
    "y = enc_sample[1:context_window + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Tokenizer decode takes a list - make sure to wrap a single value in brackets.\n",
    "\n",
    "* `tokenizer.decode(enc_sample[1])` returns an error\n",
    "* must do `tokenizer.decode([enc_sample[1]])`\n",
    "\n",
    "Let's instead decode these to get a sense of the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_window + 1):\n",
    "    print(f\"{tokenizer.decode(enc_sample[:i])} -----> {tokenizer.decode([enc_sample[i]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the dataloader and then make it into an iterator so we can get pieces of it to investigate. Notice that from the first batch to the next, we shift everything over 1 slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataload_v1(verdict, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# first batch\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second_batch\n",
    "next(data_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.7 - making embeddings\n",
    "\n",
    "This is more of less a placeholder. We produce a tensor of randomly generated small numbers based on the dimensions we need. Because we're building a vector for every word in our vocabulary, we need that number. Whatever size we want the output vector to be is important here too. \n",
    "\n",
    "So as a toy example, we bring in a 6x3 tensor. Meaning an embedding matrix for a vocabulary of 6 words of which we want the output vectors to be of length 3.\n",
    "\n",
    "We'll be optimizing these weights as part of the LLM training process later. So we're learning how to build it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to get the embeddings of 4 words - ids `2`, `3`, `5` and `1` - we use them as indices of our embedding matrix. It's just a lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-2.8400, -0.7849, -1.4096],\n",
       "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([2, 3, 5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this with more realistic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.max_token_value\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the new dimensions to create our dataloader and extract an example batch. Note we increased the batch size and made the stride equal to the max length parameter. This means there won't be overlap between examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataload_v1(verdict, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Token IDs: {inputs}\")\n",
    "print(f\"Input shape: {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a look into the actual tokens using the decoder. Note some of the odd outputs - these are the tokens that come out of BPE. `Gisburn` was broken down to `G` +  `is` + `burn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I  H AD  always \n",
      " thought  Jack  G is \n",
      "burn  rather  a  cheap \n",
      " genius -- though  a \n",
      " good  fellow  enough -- \n",
      "so  it  was  no \n",
      " great  surprise  to  me \n",
      " to  hear  that , \n"
     ]
    }
   ],
   "source": [
    "for row in inputs:\n",
    "    for elem in row:\n",
    "        print(tokenizer.decode([elem.item()]), end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training batches of IDs, we essentially append the embedding vectors of length 256. The magic of tensors starts to take shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.8 - Encoding word positions\n",
    "\n",
    "So off the rip, the embeddings above would be fine after training. However, there is a shortcoming of the LLM attention mechanism (which we'll get to in chapter 3) in that it doesn't recognize *where in the sentence* a token is, which can have a major effect on the context.\n",
    "\n",
    "There are 2 main ways to handle position\n",
    "\n",
    "* absolute - add a specific position embedding to the token embedding. For example adding 1.1 to the first token because it's the first word in the first input.\n",
    "* relative - encodes distances between words instead of exact positioning. This has a generalizability benefit.\n",
    "\n",
    "GPT uses absolute positionings that are actually optimized in the training process, so we'll be using that.\n",
    "\n",
    "All this set up needs is a second embedding object that is the same vector length for each token we expect to see if the training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we just add these things together. Apparently this just worked in the original transformer paper. Some more reading on it [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training to optimize the weights, we just need one more layer - attention! This is the most difficult part apparently. On to chapter 3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
