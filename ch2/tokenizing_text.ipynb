{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.2: tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the-verdict.txt', <http.client.HTTPMessage at 0x1067d5dd0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/\"\n",
    "       \"ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "\n",
    "urllib.request.urlretrieve(url, \"the-verdict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 20479\n",
      "First 100 characters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    verdict = f.read()\n",
    "\n",
    "\n",
    "print(f\"Number of characters: {len(verdict)}\")\n",
    "print(\"First 100 characters:\")\n",
    "verdict[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some', 'example', 'sentence', '.', 'Thanks', 'for', 'joining', 'us', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split by space\n",
    "result = re.split(r'([,.!]|\\s)', \"Some example sentence. Thanks for joining us!\")\n",
    "\n",
    "[item for item in result if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'matey',\n",
       " '!',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'some',\n",
       " 'text',\n",
       " '?',\n",
       " 'I',\n",
       " 'think',\n",
       " '?',\n",
       " 'Is',\n",
       " 'this',\n",
       " 'thing',\n",
       " '--',\n",
       " 'on',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a little more complex\n",
    "\n",
    "tmp = \"Hello, matey! Here is some text? I think? Is this thing -- on?!\"\n",
    "\n",
    "result = re.split(r'([,.:;!_\"()?]|--|\\s)', tmp)\n",
    "result = [item for item in result if item.strip()]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4690"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets apply this to the Edith Wharton text\n",
    "\n",
    "preprocessed = re.split(r'([,.:;!_\"()?\\']|--|\\s)', verdict)\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.3: Converting tokens into token IDs\n",
    "\n",
    "We have a bunch of tokens, but we need to convert them into unique IDs. Out the gate this is easy enough to do. Get the tokens and alphabetize them. Remove duplicates and gives each one a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130 unique words in the text\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "print(f\"{len(all_words)} unique words in the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx for idx, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n",
      "?: 10\n",
      "A: 11\n",
      "Ah: 12\n",
      "Among: 13\n",
      "And: 14\n",
      "Are: 15\n",
      "Arrt: 16\n",
      "As: 17\n",
      "At: 18\n",
      "Be: 19\n",
      "Begin: 20\n",
      "Burlington: 21\n"
     ]
    }
   ],
   "source": [
    "for word, idx in vocab.items():\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if idx > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all this logic into a class\n",
    "\n",
    "class defined in `my_classes.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_classes import SimpleTokenizerV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[642, 180, 663]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.encode(\"look at me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some goofy sentences using random IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forehead absurdity sign unusual\n",
      "true It resented hooded\n"
     ]
    }
   ],
   "source": [
    "print(tokenize.decode([459, 123, 888, 1050]))\n",
    "print(tokenize.decode([1035, 56, 837, 554]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.4: Adding special context tokens\n",
    "\n",
    "BUT we run into issues when a never before seen word shows up. We handle this by adding some additional handlers to the vocabulary. `|unk|` is used when we don't know a word (we can make sure the code is retrieved using the `.get` method for dicts). In addition, we can make sure we tell the model that an end of document has been reached using another one like `|end of text|`. \n",
    "\n",
    "There are many other different context tokens we can use - these are just a couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words.extend(['<|endoftext|>', '<|unk|>'])\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'younger')\n",
      "(1, 'your')\n",
      "(2, 'yourself')\n",
      "(3, '<|endoftext|>')\n",
      "(4, '<|unk|>')\n"
     ]
    }
   ],
   "source": [
    "for item in enumerate(list(vocab.keys())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_classes import SimpleTokenizerV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_v2 = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 1131]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 unknown words\n",
    "tokenize_v2.encode(\"BOOM POW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So here I am, it's in my head <|endoftext|> Eating seeds is a past time activity\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing end of text - these are 2 songs so we'd want to separate them\n",
    "\n",
    "text1 = \"So here I am, it's in my head\"\n",
    "text2 = \"Eating seeds is a past time activity\"\n",
    "\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|unk|> here I am, it' s in my head <|endoftext|> <|unk|> <|unk|> is a past time activity\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_v2.decode(tokenize_v2.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.5: Byte pair encoding (BPE)\n",
    "\n",
    "A much more sophisticated tokenization approach used in the training of the GPT series. It was first described in 1994 by Philip Gage! Quite complex to implement, so we just import it via `tiktoken`. It breaks down tokens into pieces so we can successfully embed new, unknown tokens. We don't have a bunch of `<|unk|>` tokens if we have many new words.\n",
    "\n",
    "[From wikipedia](https://en.wikipedia.org/wiki/Byte_pair_encoding), the algorithm works by finding common adjacent pairs of characters with unused placeholder bytes. It continues to do this until there are no more adjacent pairs appearing more than once or until a desired vocabulary size is reached.\n",
    "\n",
    "The process ends up with a vocabulary of every individual letter present and the various combinations that may be plentiful. [This video](https://huggingface.co/learn/nlp-course/en/chapter6/5) has a really nice step by step process of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktokenize = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496,\n",
       " 11,\n",
       " 466,\n",
       " 220,\n",
       " 345,\n",
       " 588,\n",
       " 8887,\n",
       " 30,\n",
       " 220,\n",
       " 50256,\n",
       " 554,\n",
       " 262,\n",
       " 4252,\n",
       " 18250,\n",
       " 8812,\n",
       " 2114,\n",
       " 286,\n",
       " 617,\n",
       " 20035,\n",
       " 27271,\n",
       " 13]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\"Hello, do  you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.\")\n",
    "\n",
    "integers = tiktokenize.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do  you like tea? <|endoftext|> In the sunlit terraces of someUnknownPlace.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = tiktokenize.decode(integers)\n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the nonsense word made it back - this is due to BPE algorithm that iteritively generates words.\n",
    "\n",
    "### section 2.6 Data Sampling with a Sliding Window\n",
    "\n",
    "With all of this text, we'll want to build training examples and have a way to load them into our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can this Mac use GPUs?: True\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "print(f\"Can this Mac use GPUs?: {torch.backends.mps.is_available()}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from my_classes import GPTDatasetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('the-verdict.txt', 'r', encoding='utf-8') as f:\n",
    "    verdict = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the number of tokens is slightly larger than the original approach that took the full words. This is because of the components being counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(verdict)\n",
    "len(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample for more interesting text\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the training and target sets. Our context window is the size of the text string given for context. Relatively short right now so we can get a sense of what is going on visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_window = 4\n",
    "\n",
    "x = enc_sample[:context_window]\n",
    "y = enc_sample[1:context_window + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer decode takes a list - make sure to wrap a single value in brackets.\n",
    "\n",
    "* `tokenizer.decode(enc_sample[1])` returns an error\n",
    "* must do `tokenizer.decode([enc_sample[1]])` works\n",
    "\n",
    "Let's instead decode these to get a sense of the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_window + 1):\n",
    "    print(f\"{tokenizer.decode(enc_sample[:i])} -----> {tokenizer.decode([enc_sample[i]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataload_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    '''\n",
    "    The dataset class more or less encapsulates the above sliding window logic. It breaks the text into chunks.\n",
    "    '''\n",
    "    tokenizer = tiktoken.get_encoding('gpt2')\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the dataloader and then make it into an iterator so we can get pieces of it to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataload_v1(verdict, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.7 - making embeddings\n",
    "\n",
    "This is more of less a placeholder. We generate a tensor of randomly generated small numbers based on the dimensions we need. Because we're building a vector for every word in our vocabulary, we need that number. Whatever size we want the output vector to be is important here too. \n",
    "\n",
    "So as a toy example, we bring in a 6x3 tensor.\n",
    "\n",
    "The placeholder bit is that we'll be optimizing these weights as part of the LLM training process. So we're learning how to build it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-2.8400, -0.7849, -1.4096],\n",
       "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(torch.tensor([2, 3, 5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### section 2.8 - Encoding word positions\n",
    "\n",
    "So off the rip, the embeddings above would be fine. However, there is a shortcoming of the LLM attention mechanism (which we'll get to in chapter 3) in that it doesn't recognize *where in the sentence* a token is, which can have a major effect on the context.\n",
    "\n",
    "There are 2 main ways to handle position\n",
    "\n",
    "* absolute - add a specific position embedding to the token embedding\n",
    "* relative - encodes distances between words instead of exact positioning. This has a generalizability benefit.\n",
    "\n",
    "GPT uses absolute positionings that are actually optimized in the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.max_token_value\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(vocab, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input shape: torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataload_v1(verdict, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(f\"Token IDs: {inputs}\")\n",
    "print(f\"Input shape: {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
